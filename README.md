# Interview-question-collection
收集面试问题及答案<br>

[GItHub使用指南](https://blog.csdn.net/Hanani_Jia/article/details/77950594)<br>
[GitHub 编辑指导](https://blog.csdn.net/ljc_563812704/article/details/53464039)<br>
[GItHub 公式编辑](https://www.jianshu.com/p/fd97e1f8f699)<br>
[Markdown 教程](https://hacpai.com/guide/markdown)<br>

## 计算机视觉面试问题
### [SVM](https://blog.csdn.net/v_july_v/article/details/7624837)<br>
### [CNN](https://blog.csdn.net/fengbingchun/article/details/50529500)<br>
### [softmax函数](https://blog.csdn.net/u014380165/article/details/77284921)<br>
### [attention](https://blog.csdn.net/guohao_zhang/article/details/79540014)<br>
### data augmentation
### 正则化
### 网络参数是如何计算的
### [ShuffleNet](https://blog.csdn.net/u011974639/article/details/79200559)<br>
### [最优化方法](http://www.cnblogs.com/maybe2030/p/4751804.html#_label0)<br>
1.[数学概念](https://blog.csdn.net/majinlei121/article/details/47260917)<br>
### [RNN](https://blog.csdn.net/heyongluoyao8/article/details/48636251)<br>
### [LSTM](https://blog.csdn.net/gzj_1101/article/details/79376798)<br>
### [RPN](https://blog.csdn.net/sloanqin/article/details/51545125)<br>
### PCA
### K_means
### KNN
### 损失函数
### 梯度消失和梯度弥散
### SITF
### Dropout
### Pooling
### Batch Normalization
### L1正则化和L2正则化
<br>

## 数据结构和算法问题
### [线性代数](https://www.cnblogs.com/guoyaohua/p/9803027.html)<br>
### [堆和栈的区别](https://blog.csdn.net/hairetz/article/details/4141043)<br>
### [LeetCode刷题总结](https://github.com/holyhond/Interview-question-collection/blob/master/LeetCode.md)<br>

## [计算机视觉及深度学习岗位应聘问题](https://blog.csdn.net/ferriswym/article/details/81331191)<br>
