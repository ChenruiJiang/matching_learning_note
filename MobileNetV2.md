# MobileNetV2 倒置残差和线性瓶颈  

[toc]  

[论文地址](https://arxiv.org/pdf/1801.04381.pdf)  

## 摘要  

&emsp;&emsp;在本文中，我们描述了一种新的移动网络结构MobileNetV2，它提高了移动网络在多类型任务和基准以及不同网络尺寸范围的最佳性能。我们还介绍了一种有效地使用轻量级网络进行目标检测的新颖架构SSDLite。并且我们将Deeplabv3进行修剪后构建出一种用于移动语义分割的模型，称为Mobile Deeplabv3。  
&emsp;&emsp;MobileNetV2基于倒置残差结构，并且在窄boottleneck层中有快捷连接。中间的扩大层使用轻量级的深度卷积来过滤特征作为非线性的来源。另外，我们发现在窄层中去除非线性来保持表现能力是很重要的，我们证明了这可以提高性能并且直观地启发了我们网络的设计。  
&emsp;&emsp;最后，我们的方法允许输入/输出域与转换的表达性分离，这为进一步分析提供了方便的框架。我们测量我们在ImageNet分类，COCO物体检测，VOC图像分割上的表现。我们评估准确性与乘法加法（MAdd）测量的操作次数，实际延迟和参数数量之间的权衡。  

## 1.介绍  

&emsp;&emsp;神经网络在机器智能领域有革命性地作用，比如在图像识别任务中超过了人类的识别准确率。然而，提高准确率的同时带来了新的代价：先进的网络需要的高计算能力超越了许多手机和嵌入式应用计算能力。  
&emsp;&emsp;本文介绍了一种新的为移动和资源受限环境特殊定制的神经网络结构。我们的网络在保持相同准确率的情况下通过减少计算量和内存需求，推动了移动定制计算机视觉模型的水平。  
&emsp;&emsp;我们主要贡献是一种新的层模块：具有线性瓶颈的倒置残差。这个模块采用了一种低维的压缩表示作为输入，首先扩展成高维然后使用轻量级深度卷积过滤。随后使用线性卷积将特征投影回低维表示。官方实现TensorFlow-Slim提供一部分操作。  
&emsp;&emsp;该模块可以在任何现代框架中使用标准操作有效地实现，并允许我们的模型使用标准基准测试在多个性能点上击败最新技术。此外，这种卷积模块特别适用于移动设计，因为它可以通过永远不会完全实现大型中间张量来显着减少推理期间所需的内存占用。这减少了许多嵌入式硬件设计中对主存储器访问的需求，这些设计提供了少量非常快速的软件控制的高速缓冲存储器。  

## 2.实际工作  

&emsp;&emsp;在最近几年，调整深度神经网络的架构以在准确率和性能之间有一个最佳平衡成为一个积极研究的领域。早期的网络如AlexNet,VGGNet,GoogLeNet以及ResNet，这些网络的设计都进行了大量的手工体系结构搜索和训练算法改进。最近在网络架构探索上有了很多进展，比如超参数还有各种网络修剪方法以及连通性学习。大量的工作也致力于内部卷积模块的连接架构的改变，比如ShuffleNet[20]或引入了稀疏性[21]和其它[22]。  
&emsp;&emsp;最近，[23, 24, 25, 26]引入了架构探索的新方向，将遗传算法和强化学习加入了其中。然而一个缺点是得到的网络最终会十分复杂。本文的目的是找到然后是神经网络更好地运作并用它指导最简单的网络设计。我们的方法应该被视为[23]描述方法的一种互补和相近的工作。在这种情况下，我们的方法类似于[20,22]所采用的方法并允许进一步改善性能，同时提供内部操作的一部分。我们的网络设计基于MobileNetV1。保留了它的简洁性而且不要求任何特殊操作就能显著提升准确率，实现了移动应用的多图像分类和检测任务的最新技术。  

## 3.预备、讨论和直觉  

### 3.1.深度可分离卷积  

&emsp;&emsp;深度可分离卷积是现在很多高效神经网络模块的关键部分，并且我们也将其应用到我们的网络中。基础思想是使用分解的版本代替全卷积层，将全卷积层分为深度卷积和点卷积两个部分。第一次深度卷积是使用单个卷积滤波器在每个输入通道上进行轻量级滤波。第二次点卷积使用1x1的卷积主要负责通过计算输入通道的线性组合来建立新的特征。  
&emsp;&emsp;标准卷积在输入通道为$d_{i}$输出通道为$d_{j}$，卷积和大小为$k*k$的情况下计算量为$h*w*d_{i}*d_{j}*k^2$。深度可分离卷积的计算量为$h*w*d_{i}*(k^2+d_{j})$，实验证实深度可分离卷积在性能上与常规卷积几乎相同，但计算量大致变为原来的$1/k^2$。我们使用的k为3。  

### 3.2.线性瓶颈  

&emsp;&emsp;思考一个深度神经网络由n层组成$L_{i}$，每个$L_{i}$有$h_{i}*w_{i}*d_{i}$激活张量。这节我们讨论这些张量的基础特性，我们将这些张量视为$h_{i}*w_{i}$个像素点的$d_{i}$维的容器。非正式地，对于一个真实图片的输入集合，我们说激活层集合组成了“多方面兴趣”。长期以来，人们一直以为神经网络的多方面兴趣可以嵌入到低维的子空间中。换句话说，当我们查看深度卷积层的所有单个d通道像素时，在这些值中编码的信息实际上位于某些流形中，而这些流形又可嵌入到低维子空间中。  
&emsp;&emsp;乍一看，这些事实可以简单地通过减少层的尺寸来捕获和利用从而减少操作空间的尺寸。这在MobileNetV1中成功地通过宽度乘数有效地平衡计算量和准确率。遵循这种直觉，这种宽度乘数方法允许人们减少激活空间的尺寸，直到感兴趣的流体经过整个空间。然而，当我们回想深度卷积网络实际上在每次坐标变换时都有非线性时比如ReLU，这种直觉会崩溃。例如，应用于1D空间中的线的ReLU产生“射线”，与在Rn空间中一样，它通常产生具有n关节的分段线性曲线。  
&emsp;&emsp;很容易看出，在通常情况下，ReLU转换层的结果有非零的体积S，通过对输入进行线性转换B获得测绘内部S的点，这意味部分对应于全维输出的输入空间受限于线性变换。换句话说，深度网络仅仅在非零部分具有线性分类器的能力。  
&emsp;&emsp;