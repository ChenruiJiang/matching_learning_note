[TOC]

# ShuffleNet

## 摘要

&emsp;&emsp;我们介绍一种被设计用于计算能力有限的移动装置的卷积网络ShuffleNet，新的结构使用逐点群卷积和通道混洗方式，在保持准确率的情况下极大地减少运算量。在一个基于ARM的移动装置中，ShuffleNet在和AlexNet保持相同的准确率的情况下速度提升了13倍。

## 介绍

&emsp;&emsp;目前更大更深的网络是主流趋势，主要是为了解决主流视觉识别任务。准确率最高的网络有上百层网络和上千层通道，需要的上百亿的FLOPs(每秒浮点数运算）。这篇报告与之完全对立，在保持最高的准确率的同时把计算量控制在10~100个百万FLOPs内，专注于公共移动平台，如无人机，机器人和智能手机。之前的工作主要致力于剪枝、压缩和使用小存储量代替基础网络结构。我们致力于提出一种新的适用于计算量有限制的基础网络结构。<br>
&emsp;&emsp;我们注意到现在最先进的网络如Xception和ResNeXt在小网络的作用及其有限，因为密集的1X1卷积.因此我们提出<font color=#ff000 size=3>逐点群卷积</font>来降低1x1卷积的计算复杂度，群卷积会带来一些负面作用，我们使用<font color=#ff000 size=3>通道混洗</font>来解决这个问题。基于这两种方法，我们建立了一种高效结构，命名为ShuffleNet。与流行的结构相比，我们的结构可以允许更多的特征图通道在给定计算量的前提下，能够编码更多的信息，这对小网络的表现十分重要。<br>

## 实际工作

### 高效模型的设计

&emsp;&emsp;过去几年深度神经网络在计算机视觉任务上取得了成功，其中模型设计起了很大的作用。在可嵌入设备中运行更大更深的网络的需求不断增加，这激励了高效模型设计的研究。例如，GoogleNet与简单地堆叠卷积层的网络相比，增加了网络的深度并且复杂度更低。SqueezeNet在控制准确率的情况下显著地减少了计算量和参数。ResNet使用有效的bottleneck结构来提升性能。SENet介绍了一种能够花费较小计算量来提升性能的结构单元。

### 组卷积

&emsp;&emsp;组卷积的概念是AlexNet中提出来的，当时是为了两块GPU进行运算，并且在ResNeXt中证明了其有效性。深度分离卷积在Xception中提出并且分离卷积在Inception系列中得到推广。MobileNet使用深度分离卷积并且在轻量级网络中获得了最先进的结果。我们的工作是推广群卷积和一种新形式的深度分离卷积。

### 通道混洗

&emsp;&emsp;在我们的认知中，通道混洗在高效网络模型设计中很少被提及。虽然CNNC库cuda-convnet支持随机通道卷积，这个操作相当于在组卷积后进行随机通道交换。这种“随机混洗”有不同的用途但是之后很少有人探索。最近有个two-stage卷积的工作采用了这个方法但是并没有深入研究通道混洗本身以及它在小模型设计上的作用.

### 模型加速

&emsp;&emsp;这个部分致力于加速推理同时保持预训练模型的准确率。对网络的连接和通道进行剪枝来减少网络在预训练模型的冗余连接以保模型性能。在文献中，量化和因式分解备用来减少冗余计算来加速推理。不修改参数，FFT和其它方法使用卷积方法工具来减少训练的时间消耗。把大网络的知识提炼到小网络，使训练小网络更加简单。

## 方法

### 对分组卷积的通道混洗

@import "E:\python\picture\ShuffleNet_figure 1.png" <br>
&emsp;&emsp;现在的卷积神经网络通常由相同结构组成的重复模块构成。1x1的卷积没有全部采用的原因是1x1卷积需要相当大的复杂度.在小网络中，昂贵的逐点卷积会因复杂度限制而控制通道数量，这可能显著地减少准确率。<br>
&emsp;&emsp;解决这个问题的直接方案是通道稀疏连接，比如包括1x1卷积的分组卷积。确保每个输入通道组上进行相应的卷积，组卷积就会显著地降低计算成本。但是这会带来新的问题：一些通道的输出仅仅来源于输入通道的一小部分。图1（a）说明了这种情况。很明显地看出，每个输出组只与它的输入组之间有联系。这个特性阻隔了通道组和弱表示的信息流动。<br>
&emsp;&emsp;
